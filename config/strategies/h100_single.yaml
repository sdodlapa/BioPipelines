# H100 Single GPU Strategy
# Uses Qwen2.5-72B-Instruct-AWQ on 1x H100 (80GB)
# Best quality/speed ratio for bioinformatics workflows

name: "h100_single"
description: "Single H100 GPU with Qwen2.5-72B for high-quality reasoning"

# Primary model configuration
primary_model:
  name: "Qwen/Qwen2.5-72B-Instruct-AWQ"
  provider: "vllm"
  endpoint: "http://localhost:8000/v1"
  parameters:
    max_tokens: 8192
    temperature: 0.7
    top_p: 0.95
  
# Model capabilities
capabilities:
  - reasoning
  - coding
  - tool_use
  - bioinformatics
  - workflow_generation

# SLURM configuration
slurm:
  partition: "h100flex"
  gpus: 1
  memory: "120G"
  cpus: 16
  time: "24:00:00"

# Cloud fallback (when GPU unavailable or for specialized tasks)
fallback:
  enabled: true
  providers:
    - name: "google"
      model: "gemini-2.0-flash"
      priority: 1
      description: "Free tier, 1M tokens/day"
    - name: "cerebras"
      model: "llama-3.3-70b"
      priority: 2
      description: "Free tier, fast inference"
    - name: "groq"
      model: "llama-3.3-70b-versatile"
      priority: 3
      description: "Free tier, 30 req/min"
    - name: "openrouter"
      model: "deepseek/deepseek-chat-v3-0324:free"
      priority: 4
      description: "Free DeepSeek-V3"

# Task routing (which model handles which tasks)
routing:
  workflow_generation:
    model: "primary"
    fallback: true
  code_generation:
    model: "primary"
    fallback: true
  data_search:
    model: "fallback"  # Use cloud for simple searches
    preferred_provider: "google"
  explanation:
    model: "primary"
    fallback: true

# Performance tuning
performance:
  batch_size: 1
  max_concurrent_requests: 4
  request_timeout: 120
  gpu_memory_utilization: 0.90
  max_model_len: 32768

# Monitoring
monitoring:
  log_requests: true
  log_latency: true
  health_check_interval: 30
